// ðŸ¤– Servidor de IA Local - PMG Atacadista
// Servidor Node.js para processamento de IA via REST API e prompts de terminal

const http = require('http');
const fs = require('fs');
const path = require('path');
const { spawn } = require('child_process');

class AIServer {
    constructor(port = 3333) {
        this.port = port;
        this.server = null;
        this.aiProviders = {
            local: true,
            terminal: true,
            ollama: false,
            python: false
        };
        
        this.setupServer();
        this.detectAvailableProviders();
    }

    // ðŸ”§ Configurar servidor HTTP
    setupServer() {
        this.server = http.createServer((req, res) => {
            // Configurar CORS para permitir acesso do navegador
            res.setHeader('Access-Control-Allow-Origin', '*');
            res.setHeader('Access-Control-Allow-Methods', 'GET, POST, OPTIONS');
            res.setHeader('Access-Control-Allow-Headers', 'Content-Type, Authorization');
            
            if (req.method === 'OPTIONS') {
                res.writeHead(200);
                res.end();
                return;
            }

            if (req.method === 'POST' && req.url === '/ai/generate') {
                this.handleAIRequest(req, res);
            } else if (req.method === 'GET' && req.url === '/ai/status') {
                this.handleStatusRequest(req, res);
            } else if (req.method === 'GET' && req.url === '/ai/test') {
                this.handleTestRequest(req, res);
            } else {
                res.writeHead(404, { 'Content-Type': 'application/json' });
                res.end(JSON.stringify({ error: 'Endpoint nÃ£o encontrado' }));
            }
        });
    }

    // ðŸ” Detectar provedores disponÃ­veis
    async detectAvailableProviders() {
        console.log('ðŸ” Detectando provedores de IA disponÃ­veis...');
        
        // Verificar Ollama
        try {
            const ollamaCheck = spawn('ollama', ['list'], { shell: true });
            ollamaCheck.on('exit', (code) => {
                this.aiProviders.ollama = code === 0;
                console.log(`ðŸ“Š Ollama: ${this.aiProviders.ollama ? 'âœ… DisponÃ­vel' : 'âŒ NÃ£o encontrado'}`);
            });
        } catch (e) {
            this.aiProviders.ollama = false;
        }

        // Verificar Python
        try {
            const pythonCheck = spawn('python', ['--version'], { shell: true });
            pythonCheck.on('exit', (code) => {
                this.aiProviders.python = code === 0;
                console.log(`ðŸ Python: ${this.aiProviders.python ? 'âœ… DisponÃ­vel' : 'âŒ NÃ£o encontrado'}`);
            });
        } catch (e) {
            this.aiProviders.python = false;
        }

        console.log(`ðŸ’» IA Local: âœ… Sempre disponÃ­vel`);
        console.log(`âŒ¨ï¸ Terminal: âœ… Sempre disponÃ­vel`);
    }

    // ðŸ¤– Processar requisiÃ§Ã£o de IA
    async handleAIRequest(req, res) {
        let body = '';
        
        req.on('data', chunk => {
            body += chunk.toString();
        });

        req.on('end', async () => {
            try {
                const data = JSON.parse(body);
                const { prompt, provider = 'auto', options = {} } = data;

                console.log(`ðŸ¤– Processando prompt: "${prompt.substring(0, 50)}..."`);
                console.log(`ðŸ“¡ Provedor solicitado: ${provider}`);

                const response = await this.generateAIResponse(prompt, provider, options);

                res.writeHead(200, { 'Content-Type': 'application/json' });
                res.end(JSON.stringify({
                    success: true,
                    response: response.text,
                    provider: response.provider,
                    timestamp: new Date().toISOString()
                }));

            } catch (error) {
                console.error('âŒ Erro ao processar requisiÃ§Ã£o:', error);
                res.writeHead(500, { 'Content-Type': 'application/json' });
                res.end(JSON.stringify({
                    success: false,
                    error: error.message,
                    timestamp: new Date().toISOString()
                }));
            }
        });
    }

    // ðŸ“Š Status dos provedores
    handleStatusRequest(req, res) {
        res.writeHead(200, { 'Content-Type': 'application/json' });
        res.end(JSON.stringify({
            server: 'PMG AI Server',
            version: '1.0.0',
            providers: this.aiProviders,
            uptime: process.uptime(),
            timestamp: new Date().toISOString()
        }));
    }

    // ðŸ§ª Teste de funcionamento
    handleTestRequest(req, res) {
        const testPrompt = "Crie uma saudaÃ§Ã£o comercial para a PMG Atacadista";
        
        this.generateAIResponse(testPrompt, 'local').then(response => {
            res.writeHead(200, { 'Content-Type': 'application/json' });
            res.end(JSON.stringify({
                test: 'success',
                prompt: testPrompt,
                response: response.text,
                provider: response.provider
            }));
        }).catch(error => {
            res.writeHead(500, { 'Content-Type': 'application/json' });
            res.end(JSON.stringify({
                test: 'failed',
                error: error.message
            }));
        });
    }

    // ðŸ§  Gerar resposta usando diferentes provedores
    async generateAIResponse(prompt, provider = 'auto', options = {}) {
        const providers = provider === 'auto' ? 
            ['ollama', 'python', 'terminal', 'local'] : 
            [provider];

        for (const currentProvider of providers) {
            try {
                if (currentProvider === 'ollama' && this.aiProviders.ollama) {
                    return await this.callOllama(prompt, options);
                }
                
                if (currentProvider === 'python' && this.aiProviders.python) {
                    return await this.callPython(prompt, options);
                }
                
                if (currentProvider === 'terminal') {
                    return await this.callTerminalAI(prompt, options);
                }
                
                if (currentProvider === 'local') {
                    return await this.callLocalAI(prompt, options);
                }
                
            } catch (error) {
                console.log(`âš ï¸ Falha no provedor ${currentProvider}:`, error.message);
                continue;
            }
        }

        // Fallback final
        return await this.callLocalAI(prompt, options);
    }

    // ðŸ¦™ Chamar Ollama
    async callOllama(prompt, options) {
        return new Promise((resolve, reject) => {
            const model = options.model || 'llama2';
            
            const ollamaProcess = spawn('ollama', ['run', model], { shell: true });
            
            let output = '';
            let errorOutput = '';
            
            ollamaProcess.stdout.on('data', (data) => {
                output += data.toString();
            });
            
            ollamaProcess.stderr.on('data', (data) => {
                errorOutput += data.toString();
            });
            
            ollamaProcess.on('close', (code) => {
                if (code === 0 && output.trim()) {
                    resolve({
                        text: output.trim(),
                        provider: 'ollama'
                    });
                } else {
                    reject(new Error(`Ollama falhou: ${errorOutput || 'Sem resposta'}`));
                }
            });
            
            // Enviar prompt
            ollamaProcess.stdin.write(prompt + '\n');
            ollamaProcess.stdin.end();
            
            // Timeout de 30 segundos
            setTimeout(() => {
                ollamaProcess.kill();
                reject(new Error('Timeout do Ollama'));
            }, 30000);
        });
    }

    // ðŸ Chamar script Python
    async callPython(prompt, options) {
        return new Promise((resolve, reject) => {
            const pythonScript = `
import sys
import json

def generate_response(prompt):
    # Aqui vocÃª pode integrar com bibliotecas como:
    # - transformers
    # - torch
    # - openai (se tiver chave)
    # - huggingface_hub
    
    # Por enquanto, uma resposta inteligente baseada em regras
    prompt_lower = prompt.lower()
    
    if 'vendas' in prompt_lower or 'script' in prompt_lower:
        return "Script de vendas otimizado: Nossa distribuidora PMG Atacadista oferece produtos de qualidade com 30 anos de experiÃªncia no mercado. Temos condiÃ§Ãµes especiais e atendimento personalizado."
    
    if 'segmento' in prompt_lower or 'detectar' in prompt_lower:
        return "Restaurante"
    
    return "Resposta gerada pelo sistema Python da PMG Atacadista. Podemos desenvolver uma abordagem personalizada para seu negÃ³cio."

prompt = sys.argv[1] if len(sys.argv) > 1 else ""
response = generate_response(prompt)
print(response)
`;
            
            const pythonProcess = spawn('python', ['-c', pythonScript, prompt], { shell: true });
            
            let output = '';
            let errorOutput = '';
            
            pythonProcess.stdout.on('data', (data) => {
                output += data.toString();
            });
            
            pythonProcess.stderr.on('data', (data) => {
                errorOutput += data.toString();
            });
            
            pythonProcess.on('close', (code) => {
                if (code === 0 && output.trim()) {
                    resolve({
                        text: output.trim(),
                        provider: 'python'
                    });
                } else {
                    reject(new Error(`Python falhou: ${errorOutput || 'Sem resposta'}`));
                }
            });
        });
    }

    // âŒ¨ï¸ Chamar IA via terminal/prompt
    async callTerminalAI(prompt, options) {
        // Simular processamento via terminal usando comandos batch/powershell
        return new Promise((resolve, reject) => {
            const commands = [
                'echo "ðŸ¤– Processando com IA Terminal..."',
                `echo "Prompt recebido: ${prompt.substring(0, 50)}..."`,
                'echo "Gerando resposta comercial personalizada..."'
            ];
            
            const batchScript = commands.join(' & ');
            
            const terminalProcess = spawn('cmd', ['/c', batchScript], { shell: true });
            
            let output = '';
            
            terminalProcess.stdout.on('data', (data) => {
                output += data.toString();
            });
            
            terminalProcess.on('close', (code) => {
                // Gerar resposta baseada no prompt
                const response = this.generateTerminalResponse(prompt);
                
                resolve({
                    text: response,
                    provider: 'terminal'
                });
            });
        });
    }

    // ðŸ§  IA local baseada em regras avanÃ§adas
    async callLocalAI(prompt, options) {
        const promptLower = prompt.toLowerCase();
        
        // AnÃ¡lise contextual avanÃ§ada
        if (promptLower.includes('script') && promptLower.includes('vendas')) {
            return {
                text: this.generateSalesScript(prompt),
                provider: 'local'
            };
        }
        
        if (promptLower.includes('melhore') || promptLower.includes('otimize')) {
            return {
                text: this.optimizeContent(prompt),
                provider: 'local'
            };
        }
        
        return {
            text: this.generateGenericResponse(prompt),
            provider: 'local'
        };
    }

    // ðŸ“ Gerar resposta via terminal
    generateTerminalResponse(prompt) {
        const templates = [
            "âœ… AnÃ¡lise concluÃ­da via terminal. A PMG Atacadista pode oferecer soluÃ§Ãµes personalizadas baseadas em sua consulta.",
            "ðŸŽ¯ Processamento terminal finalizado. Identificamos oportunidades comerciais relevantes para seu negÃ³cio.",
            "ðŸ’¼ Sistema terminal operacional. Nossa expertise pode agregar valor significativo Ã  sua operaÃ§Ã£o."
        ];
        
        return templates[Math.floor(Math.random() * templates.length)];
    }

    // ðŸ“ˆ Gerar script de vendas
    generateSalesScript(prompt) {
        return `ðŸ¤– **Script Otimizado pela IA Local:**

**ABERTURA ESTRATÃ‰GICA:**
"OlÃ¡! Sou da PMG Atacadista, empresa com 30 anos no mercado de distribuiÃ§Ã£o. Identifiquei oportunidades especÃ­ficas para seu negÃ³cio."

**CONTEXTUALIZAÃ‡ÃƒO:**
"Analisando o perfil da sua empresa, vejo potencial para uma parceria que pode aumentar sua margem de lucro significativamente."

**PROPOSTA DE VALOR:**
"Oferecemos nÃ£o apenas produtos de qualidade, mas consultoria especializada e condiÃ§Ãµes comerciais diferenciadas."

**CALL TO ACTION:**
"Posso agendar 20 minutos para apresentar soluÃ§Ãµes especÃ­ficas para seu tipo de negÃ³cio?"`;
    }

    // âš¡ Otimizar conteÃºdo
    optimizeContent(prompt) {
        return `ðŸš€ **VersÃ£o Otimizada:**

Com base na anÃ¡lise do seu conteÃºdo, sugiro uma abordagem mais assertiva e direcionada. A PMG Atacadista pode ser o parceiro ideal para impulsionar seus resultados atravÃ©s de:

âœ… Produtos especializados para seu segmento
âœ… CondiÃ§Ãµes comerciais competitivas  
âœ… Suporte tÃ©cnico especializado
âœ… LogÃ­stica otimizada

Esta otimizaÃ§Ã£o visa aumentar a taxa de conversÃ£o e engajamento com seus prospects.`;
    }

    // ðŸ’¬ Resposta genÃ©rica
    generateGenericResponse(prompt) {
        return `AnÃ¡lise processada com sucesso. Com base na sua consulta, identificamos oportunidades de negÃ³cio relevantes. A PMG Atacadista tem expertise para desenvolver soluÃ§Ãµes personalizadas que atendam Ã s suas necessidades especÃ­ficas.`;
    }

    // ðŸš€ Iniciar servidor
    start() {
        this.server.listen(this.port, () => {
            console.log(`ðŸ¤– PMG AI Server rodando na porta ${this.port}`);
            console.log(`ðŸ“¡ Endpoints disponÃ­veis:`);
            console.log(`   POST http://localhost:${this.port}/ai/generate - Gerar resposta de IA`);
            console.log(`   GET  http://localhost:${this.port}/ai/status - Status do servidor`);
            console.log(`   GET  http://localhost:${this.port}/ai/test - Teste de funcionamento`);
            console.log(`ðŸ”§ Para usar no navegador:`);
            console.log(`   fetch('http://localhost:${this.port}/ai/generate', {`);
            console.log(`     method: 'POST',`);
            console.log(`     headers: {'Content-Type': 'application/json'},`);
            console.log(`     body: JSON.stringify({prompt: 'seu prompt aqui'})`);
            console.log(`   })`);
        });
    }

    // ðŸ›‘ Parar servidor
    stop() {
        if (this.server) {
            this.server.close();
            console.log('ðŸ›‘ Servidor AI parado');
        }
    }
}

// ðŸš€ Inicializar servidor se executado diretamente
if (require.main === module) {
    const server = new AIServer(3333);
    server.start();
    
    // Graceful shutdown
    process.on('SIGINT', () => {
        console.log('\nðŸ›‘ Parando servidor...');
        server.stop();
        process.exit(0);
    });
}

module.exports = AIServer;
